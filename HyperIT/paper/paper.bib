@article{ayrolles_hypyp_2021,
	title = {{HyPyP}: {A} {Hyperscanning} {Python} {Pipeline} for inter-brain connectivity analysis},
	volume = {16},
	issn = {1749-5016, 1749-5024},
	shorttitle = {{HyPyP}},
	url = {https://academic.oup.com/scan/article/16/1-2/72/5919711},
	doi = {10.1093/scan/nsaa141},
	abstract = {Abstract
            The bulk of social neuroscience takes a ‘stimulus-brain’ approach, typically comparing brain responses to different types of social stimuli, but most of the time in the absence of direct social interaction. Over the last two decades, a growing number of researchers have adopted a ‘brain-to-brain’ approach, exploring similarities between brain patterns across participants as a novel way to gain insight into the social brain. This methodological shift has facilitated the introduction of naturalistic social stimuli into the study design (e.g. movies) and, crucially, has spurred the development of new tools to directly study social interaction, both in controlled experimental settings and in more ecologically valid environments. Specifically, ‘hyperscanning’ setups, which allow the simultaneous recording of brain activity from two or more individuals during social tasks, has gained popularity in recent years. However, currently, there is no agreed-upon approach to carry out such ‘inter-brain connectivity analysis’, resulting in a scattered landscape of analysis techniques. To accommodate a growing demand to standardize analysis approaches in this fast-growing research field, we have developed Hyperscanning Python Pipeline, a comprehensive and easy open-source software package that allows (social) neuroscientists to carry-out and to interpret inter-brain connectivity analyses.},
	language = {en},
	number = {1-2},
	urldate = {2024-02-06},
	journal = {Social Cognitive and Affective Neuroscience},
	author = {Ayrolles, Anaël and Brun, Florence and Chen, Phoebe and Djalovski, Amir and Beauxis, Yann and Delorme, Richard and Bourgeron, Thomas and Dikker, Suzanne and Dumas, Guillaume},
	month = jan,
	year = {2021},
	pages = {72--83},
}

@article{barnett_granger_2009,
	title = {Granger causality and transfer entropy are equivalent for {Gaussian} variables},
	volume = {103},
	copyright = {http://link.aps.org/licenses/aps-default-license},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.103.238701},
	doi = {10.1103/PhysRevLett.103.238701},
	language = {en},
	number = {23},
	urldate = {2024-04-08},
	journal = {Physical Review Letters},
	author = {Barnett, Lionel and Barrett, Adam B. and Seth, Anil K.},
	month = dec,
	year = {2009},
	pages = {238701},
}

@article{dumas_inter-brain_2010,
	title = {Inter-brain synchronization during social interaction},
	volume = {5},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0012166},
	doi = {10.1371/journal.pone.0012166},
	language = {en},
	number = {8},
	urldate = {2024-02-12},
	journal = {PLoS ONE},
	author = {Dumas, Guillaume and Nadel, Jacqueline and Soussignan, Robert and Martinerie, Jacques and Garnero, Line},
	editor = {Lauwereyns, Jan},
	month = aug,
	year = {2010},
	pages = {e12166},
	file = {Full Text:/Users/edoardochidichimo/Zotero/storage/MX7V2XN2/Dumas et al. - 2010 - Inter-Brain Synchronization during Social Interact.pdf:application/pdf},
}


@article{lizier_jidt_2014,
	title = {{JIDT}: {An} information-theoretic toolkit for studying the dynamics of complex systems},
	volume = {1},
	issn = {2296-9144},
	shorttitle = {{JIDT}},
	url = {http://journal.frontiersin.org/article/10.3389/frobt.2014.00011/abstract},
	doi = {10.3389/frobt.2014.00011},
	urldate = {2024-04-08},
	journal = {Frontiers in Robotics and AI},
	author = {Lizier, Joseph T.},
	month = dec,
	year = {2014},
}

@article{luppi_synergistic_2022,
	title = {A synergistic core for human brain evolution and cognition},
	volume = {25},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/s41593-022-01070-0},
	doi = {10.1038/s41593-022-01070-0},
	language = {en},
	number = {6},
	urldate = {2023-12-04},
	journal = {Nature Neuroscience},
	author = {Luppi, Andrea I. and Mediano, Pedro A. M. and Rosas, Fernando E. and Holland, Negin and Fryer, Tim D. and O’Brien, John T. and Rowe, James B. and Menon, David K. and Bor, Daniel and Stamatakis, Emmanuel A.},
	month = jun,
	year = {2022},
	pages = {771--782},
}

@article{mediano_beyond_2019,
	title = {Beyond integrated information: {A} taxonomy of information dynamics phenomena},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Beyond integrated information},
	url = {https://arxiv.org/abs/1909.02297},
	doi = {10.48550/ARXIV.1909.02297},
	abstract = {Most information dynamics and statistical causal analysis frameworks rely on the common intuition that causal interactions are intrinsically pairwise -- every 'cause' variable has an associated 'effect' variable, so that a 'causal arrow' can be drawn between them. However, analyses that depict interdependencies as directed graphs fail to discriminate the rich variety of modes of information flow that can coexist within a system. This, in turn, creates problems with attempts to operationalise the concepts of 'dynamical complexity' or `integrated information.' To address this shortcoming, we combine concepts of partial information decomposition and integrated information, and obtain what we call Integrated Information Decomposition, or \$Φ\$ID. We show how \$Φ\$ID paves the way for more detailed analyses of interdependencies in multivariate time series, and sheds light on collective modes of information dynamics that have not been reported before. Additionally, \$Φ\$ID reveals that what is typically referred to as 'integration' is actually an aggregate of several heterogeneous phenomena. Furthermore, \$Φ\$ID can be used to formulate new, tailored measures of integrated information, as well as to understand and alleviate the limitations of existing measures.},
	urldate = {2024-03-01},
	author = {Mediano, Pedro A. M. and Rosas, Fernando and Carhart-Harris, Robin L. and Seth, Anil K. and Barrett, Adam B.},
	year = {2019},
	note = {Publisher: [object Object]
Version Number: 1},
	keywords = {Data Analysis, Statistics and Probability (physics.data-an), FOS: Biological sciences, FOS: Physical sciences, Neurons and Cognition (q-bio.NC)},
}

@article{mediano_towards_2021,
	title = {Towards an extended taxonomy of information dynamics via {Integrated} {Information} {Decomposition}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2109.13186},
	doi = {10.48550/ARXIV.2109.13186},
	abstract = {Complex systems, from the human brain to the global economy, are made of multiple elements that interact in such ways that the behaviour of the `whole' often seems to be more than what is readily explainable in terms of the `sum of the parts.' Our ability to understand and control these systems remains limited, one reason being that we still don't know how best to describe -- and quantify -- the higher-order dynamical interactions that characterise their complexity. To address this limitation, we combine principles from the theories of Information Decomposition and Integrated Information into what we call Integrated Information Decomposition, or \$Φ\$ID. \$Φ\$ID provides a comprehensive framework to reason about, evaluate, and understand the information dynamics of complex multivariate systems. \$Φ\$ID reveals the existence of previously unreported modes of collective information flow, providing tools to express well-known measures of information transfer and dynamical complexity as aggregates of these modes. Via computational and empirical examples, we demonstrate that \$Φ\$ID extends our explanatory power beyond traditional causal discovery methods -- with profound implications for the study of complex systems across disciplines.},
	urldate = {2023-12-04},
	author = {Mediano, Pedro A. M. and Rosas, Fernando E. and Luppi, Andrea I and Carhart-Harris, Robin L. and Bor, Daniel and Seth, Anil K. and Barrett, Adam B.},
	year = {2021},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Data Analysis, Statistics and Probability (physics.data-an), FOS: Biological sciences, FOS: Physical sciences, Neurons and Cognition (q-bio.NC)},
}

@article{shannon_mathematical_1948,
	title = {A mathematical theory of communication},
	volume = {27},
	issn = {00058580},
	url = {https://ieeexplore.ieee.org/document/6773024},
	doi = {10.1002/j.1538-7305.1948.tb01338.x},
	language = {en},
	number = {3},
	urldate = {2023-12-14},
	journal = {Bell System Technical Journal},
	author = {Shannon, C. E.},
	month = jul,
	year = {1948},
	pages = {379--423},
}

@article{timme_tutorial_2018,
	title = {A tutorial for information theory in neuroscience},
	volume = {5},
	issn = {2373-2822},
	url = {https://www.eneuro.org/lookup/doi/10.1523/ENEURO.0052-18.2018},
	doi = {10.1523/ENEURO.0052-18.2018},
	abstract = {Abstract
            Understanding how neural systems integrate, encode, and compute information is central to understanding brain function. Frequently, data from neuroscience experiments are multivariate, the interactions between the variables are nonlinear, and the landscape of hypothesized or possible interactions between variables is extremely broad. Information theory is well suited to address these types of data, as it possesses multivariate analysis tools, it can be applied to many different types of data, it can capture nonlinear interactions, and it does not require assumptions about the structure of the underlying data (i.e., it is model independent). In this article, we walk through the mathematics of information theory along with common logistical problems associated with data type, data binning, data quantity requirements, bias, and significance testing. Next, we analyze models inspired by canonical neuroscience experiments to improve understanding and demonstrate the strengths of information theory analyses. To facilitate the use of information theory analyses, and an understanding of how these analyses are implemented, we also provide a free MATLAB software package that can be applied to a wide range of data from neuroscience experiments, as well as from other fields of study.},
	language = {en},
	number = {3},
	urldate = {2023-12-13},
	journal = {eneuro},
	author = {Timme, Nicholas M. and Lapish, Christopher},
	month = may,
	year = {2018},
	pages = {ENEURO.0052--18.2018},
}

@article{williams_nonnegative_2010,
	title = {Nonnegative decomposition of multivariate information},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1004.2515},
	doi = {10.48550/ARXIV.1004.2515},
	abstract = {Of the various attempts to generalize information theory to multiple variables, the most widely utilized, interaction information, suffers from the problem that it is sometimes negative. Here we reconsider from first principles the general structure of the information that a set of sources provides about a given variable. We begin with a new definition of redundancy as the minimum information that any source provides about each possible outcome of the variable, averaged over all possible outcomes. We then show how this measure of redundancy induces a lattice over sets of sources that clarifies the general structure of multivariate information. Finally, we use this redundancy lattice to propose a definition of partial information atoms that exhaustively decompose the Shannon information in a multivariate system in terms of the redundancy between synergies of subsets of the sources. Unlike interaction information, the atoms of our partial information decomposition are never negative and always support a clear interpretation as informational quantities. Our analysis also demonstrates how the negativity of interaction information can be explained by its confounding of redundancy and synergy.},
	urldate = {2024-02-19},
	author = {Williams, Paul L. and Beer, Randall D.},
	year = {2010},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Data Analysis, Statistics and Probability (physics.data-an), FOS: Biological sciences, FOS: Physical sciences, Neurons and Cognition (q-bio.NC), FOS: Computer and information sciences, 94A15, Biological Physics (physics.bio-ph), Information Theory (cs.IT), Mathematical Physics (math-ph), Quantitative Methods (q-bio.QM)},
}